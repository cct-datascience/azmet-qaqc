---
title: "azmet-qaqc"
format: 
  gfm:
    toc: true
    number-sections: true
    
editor: visual
bibliography: references.bib
---

```{r setup}
#| echo: true
#| code-fold: false
#| warning: false
#| message: false

# remotes::install_github("cct-datascience/azmetr")
library(azmetr)
library(tsibble)
library(tidyverse)
library(lubridate)
library(feasts)
library(fable)
library(slider)
```

# Problem Definition

We want to use forecasts to do quality assurance of AZMet weather data.
Use the existing timeseries available from the API (and possibly also historical data not on the API) to forecast the current day's (or hour's) data with prediction interval(s).
Data that falls outside of those prediction interval(s) will get flagged as extreme values and possibly interpolated.
Variables that need QA include:

-   precipitation
-   air temperature
-   soil temperature
-   solar radiation
-   wind speed
-   humidity

Other variables are (probably?) derived

# Gathering Information

## Retrieve Data

Download all the daily data available from the API.

```{r import-data}
#| cache: true
#| code-fold: false

#read in historic data
daily_hist <- read_csv("data/daily_hist.csv")

#get data from end of historic data until 2022-10-19
if(!file.exists("data/daily-2022-10-19.rds")) {
  daily <- az_daily(start_date = max(daily_hist$datetime) + 1,
                    end_date = "2022-10-19")
  
  #save these data
  write_rds(daily, "data/daily-2022-10-19.rds")
}

# Read in already existing data, extract last date, retrieve data since that date
daily_old <- read_rds("data/daily-2022-10-19.rds")
lastdate <- max(daily_old$datetime)

daily_new <- az_daily(start_date = lastdate + 1)

daily <- bind_rows(daily_old, daily_new)

#join to historic data

daily <- bind_rows(daily_hist, daily) 
```

Find and resolve duplicates

```{r}
duplicates(daily, key = c(meta_station_id, meta_station_name), index = datetime)

daily <- daily |>
  filter(!are_duplicated(
    daily,
    key = c(meta_station_id, meta_station_name),
    index = datetime
  ))
```

Convert to tsibble for exploration

```{r}
#| code-fold: false
daily_ts <- as_tsibble(daily, key = c(meta_station_id, meta_station_name), index = datetime)
```

Any rows marked as needing review?

```{r}
daily_ts |> filter(meta_needs_review != 0)
```


Check extreme values:

```{r}
daily_ts |> filter(temp_air_meanC > 200)
```

Looks like historic data uses 999 for NA probably. I'll set those to `NA`

```{r}
daily_ts <- daily_ts |> 
  mutate(across(where(is.numeric), \(x) if_else(x == 999, NA_real_, x)))
```



Any gaps in the data?

```{r}
daily_ts |> scan_gaps()
```

yes, let's make them explicit NAs

```{r}
daily_ts <- fill_gaps(daily_ts, .full = TRUE)
```


Check that gaps were made explicit:

```{r}
#| code-fold: true
daily_ts |> 
  filter(meta_station_id %in% c("az23", "az14"))|>
  filter(year(datetime) > 2018) |> 
  ggplot(aes(x = datetime, y = sol_rad_total)) +
  geom_line(na.rm = TRUE, size = 0.1) +
  geom_point(na.rm = TRUE, size = 0.2) +
  facet_wrap(~meta_station_id)
```

# Preliminary (exploratory) analysis

I'll start by looking at a subset of sites just to make visualization easier.

```{r}
daily_ts_sub <-
  daily_ts |> 
  filter(meta_station_name %in% c("Aguila", "Harquahala", "Tucson", "Maricopa"))
```

Is there missing data?

```{r}
#| fig-height: 10
#| fig-width: 10

daily_ts_sub |> 
  as_tibble() |>
  group_by(meta_station_id, meta_station_name) |> 
  summarize(across(everything(), ~sum(is.na(.))))

```

Some variables are incomplete or have a short time gap.
This may make forecasting difficult.



## Mean Air Temp

```{r}
daily_ts_sub |> 
  autoplot(temp_air_meanC) +
  labs(title = "Mean Air Temp (ºC)")
```

Seasonality

```{r}
daily_ts_sub |> gg_season(temp_air_meanC) + labs(title = "Mean Air Temp (ºC)")
```

Autocorrelation

```{r}
daily_ts_sub |>
  ACF(temp_air_meanC, lag_max = 180) |> 
  autoplot()
```

## Solar Radiation

```{r}
daily_ts_sub |> autoplot(sol_rad_total) + labs(title = "Total Solar Radiation")
```

Seasonality

```{r}
daily_ts_sub |> gg_season(sol_rad_total) + labs(title = "Total Solar Radiation")
```

Definitely some weird zeroes.
Maybe a super cloudy day, but probably errors.

```{r}
#| code-fold: true
daily_ts |> 
  filter(sol_rad_total < 1 & !is.na(sol_rad_total)) |> 
  select(datetime, sol_rad_total, meta_station_id, meta_needs_review) |> 
  arrange(sol_rad_total)
```

Autocorrelation

```{r}
daily_ts_sub |>
  ACF(sol_rad_total, lag_max = 180) |> 
  autoplot()
```

## Precipitation

```{r}
daily_ts_sub |> autoplot(precip_total_mm) + labs(title = "Precip (mm)")
```

Seasonality

```{r}
daily_ts_sub |> gg_season(precip_total_mm) + labs(title = "Precip (mm)")
```

Autocorrelation

```{r}
daily_ts_sub |>
  ACF(precip_total_mm, lag_max = 180) |> 
  autoplot()
```

## Combinations of variables

Is total solar radiation largely a function of precipitation?

```{r}
daily_ts_sub |> 
  ggplot(aes(x = precip_total_mm, y = sol_rad_total)) +
  geom_point() +
  facet_wrap(~meta_station_id + meta_station_name)
```

Not much relationship

```{r}
#| warning: false
#| message: false
daily_ts_sub |>
  GGally::ggpairs(
    columns = c(
      "eto_pen_mon_in",
      "precip_total_mm",
      "sol_rad_total",
      "temp_air_meanC"#,
      # "temp_soil_50cm_meanC",
      # "vp_actual_mean",
      # "wind_spd_mean_mph"
    )
  )

```

# Choosing and fitting models

## Sliding windows

I'll explore the sliding window quantile approach used in [@faybishenko2021].
I'll use `slider` because it's tidyverse-friendly and I want to learn it.
I'll use a rolling, centered, 6-month window to calculate upper and lower 95% and 99% quantiles.
Anything outside of the rolling 99% quantile is an "extreme" value, which @faybishenko2021 assumed was bad data, and anything between the 95% and 99% quantile is an "outlier" which is suspect, but not necessarily bad.

In the plots below, I only show a few stations for clarity.

### Mean Air Temp

```{r}
#| code-fold: true
temp_roll_test <-
  daily_ts |> 
  select(temp_air_meanC) |> 
  group_by_key() |> 
  mutate(
    rolling_median = slide_dbl(
      temp_air_meanC,
      median,
      .before = 90,
      .after  = 90,
      .complete = TRUE
    )
  ) |>
  mutate(upper95 = slide_dbl(
    temp_air_meanC,
    \(x) quantile(x, c(0.975), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |>
  mutate(lower95 = slide_dbl(
    temp_air_meanC,
    \(x) quantile(x, c(0.025), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(upper99 = slide_dbl(
    temp_air_meanC,
    \(x) quantile(x, c(0.999), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(lower99 = slide_dbl(
    temp_air_meanC,
    \(x) quantile(x, c(0.001), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(QA = case_when(
    temp_air_meanC > lower99 & temp_air_meanC < lower95 ~ "outlier",
    temp_air_meanC > upper95 & temp_air_meanC < upper99 ~ "outlier",
    temp_air_meanC < lower99 ~ "extreme",
    temp_air_meanC > upper99 ~ "extreme",
    is.na(temp_air_meanC) ~ NA_character_
  )) 
```

```{r}
#| code-fold: true
temp_roll_test |> 
 filter(meta_station_name %in% c("Bowie", "Harquahala", "Tucson", "Maricopa")) |>
  ggplot(aes(x = datetime)) +
  geom_point(aes(y = temp_air_meanC, color = QA), size = 0.5, alpha = 0.5) +
  geom_line(aes(y = rolling_median)) +
  geom_ribbon(aes(ymin = lower95, ymax = upper95), alpha = 0.3) +
  geom_ribbon(aes(ymin = lower99, ymax = upper99), alpha = 0.3) +
  facet_wrap(~meta_station_name)
```

### Solar radiation

```{r}
#| code-fold: true

sol_roll_test <-
  daily_ts |> 
  group_by_key() |>
  select(sol_rad_total) |>
  mutate(
    sol_rolling_median = slide_dbl(
      sol_rad_total,
      median,
      .before = 90,
      .after  = 90,
      .complete = TRUE
    )
  ) |>
  mutate(sol_upper95 = slide_dbl(
    sol_rad_total,
    \(x) quantile(x, c(0.975), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |>
  mutate(sol_lower95 = slide_dbl(
    sol_rad_total,
    \(x) quantile(x, c(0.025), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(sol_upper99 = slide_dbl(
    sol_rad_total,
    \(x) quantile(x, c(0.999), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(sol_lower99 = slide_dbl(
    sol_rad_total,
    \(x) quantile(x, c(0.001), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(QA = case_when(
    sol_rad_total > sol_lower99 & sol_rad_total < sol_lower95 ~ "outlier",
    sol_rad_total > sol_upper95 & sol_rad_total < sol_upper99 ~ "outlier",
    sol_rad_total < sol_lower99 ~ "extreme",
    sol_rad_total > sol_upper99 ~ "extreme",
    is.na(sol_rad_total) ~ NA_character_
  )) 

```

```{r}
#| code-fold: true
sol_roll_test |> 
 filter(meta_station_name %in% c("Bowie", "Harquahala", "Tucson", "Maricopa")) |>
  ggplot(aes(x = datetime)) +
  geom_point(aes(y = sol_rad_total, color = QA), size = 0.5) +
  geom_line(aes(y = sol_rolling_median)) +
  geom_ribbon(aes(ymin = sol_lower95, ymax = sol_upper95), alpha = 0.3) +
  geom_ribbon(aes(ymin = sol_lower99, ymax = sol_upper99), alpha = 0.3) +
  facet_wrap(~meta_station_name)
```

### Precipitation

```{r}
#| code-fold: true
precip_roll_test <-
  daily_ts |> 
  select(precip_total_mm) |> 
  group_by_key() |> 
  mutate(
    precip_rolling_median = slide_dbl(
      precip_total_mm,
      median,
      .before = 90,
      .after  = 90,
      .complete = TRUE
    )
  ) |>
  mutate(precip_upper95 = slide_dbl(
    precip_total_mm,
    \(x) quantile(x, c(0.975), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |>
  mutate(precip_lower95 = slide_dbl(
    precip_total_mm,
    \(x) quantile(x, c(0.025), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(precip_upper99 = slide_dbl(
    precip_total_mm,
    \(x) quantile(x, c(0.999), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(precip_lower99 = slide_dbl(
    precip_total_mm,
    \(x) quantile(x, c(0.001), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(QA = case_when(
    precip_total_mm > precip_lower99 & precip_total_mm < precip_lower95 ~ "outlier",
    precip_total_mm > precip_upper95 & precip_total_mm < precip_upper99 ~ "outlier",
    precip_total_mm < precip_lower99 ~ "extreme",
    precip_total_mm > precip_upper99 ~ "extreme",
    is.na(precip_total_mm) ~ NA_character_
  )) 
```

```{r}
#| code-fold: true
precip_roll_test |> 
 filter(meta_station_name %in% c("Bowie", "Harquahala", "Tucson", "Maricopa")) |>
  ggplot(aes(x = datetime)) +
  geom_point(aes(y = precip_total_mm, color = QA), size = 0.5) +
  # geom_line(aes(y = precip_rolling_median)) +
  geom_ribbon(aes(ymin = precip_lower95, ymax = precip_upper95), alpha = 0.3) +
  geom_ribbon(aes(ymin = precip_lower99, ymax = precip_upper99), alpha = 0.3) +
  facet_wrap(~meta_station_name)
```
Seems a little overzealous with precip.  Likely because an emprical distribution is not really appropriate for precip data.


## Forecasting

### Timeseries decomposition

Timeseries decomposition doesn't work when there is missing data.
There are `NA`s for Harquahala and Bowie stations, so I guess I'll have to omit those until I figure out how to do this with missing data.

```{r}
daily_ts_sub |> filter(is.na(sol_rad_total))
```

```{r}
dcmp <- 
  daily_ts_sub |> 
  # filter(meta_station_id == first(meta_station_id)) |> 
  model(stl = STL(sol_rad_total, iterations = 3)) |> 
  filter(meta_station_id %in% c("az01", "az06"))
components(dcmp)
```

```{r}
components(dcmp) |>
  as_tsibble() |>
  autoplot(sol_rad_total, color = "grey") +
  facet_wrap(~meta_station_id + meta_station_name, ncol = 1) +
  geom_line(aes(y = trend, color = "trend")) 

components(dcmp) |> autoplot()
```

Hmm... I feel like trend should be smoother and `season_week` should be `season_month` or something instead.

```{r}
components(dcmp) |> 
  as_tsibble() |> 
  autoplot(sol_rad_total, color = "grey")
  
```
