---
title: "azmet-qaqc"
format: 
  gfm:
    toc: true
    number-sections: true
    
editor: visual
bibliography: references.bib
---

```{r setup}
#| echo: true
#| code-fold: false
#| warning: false
#| message: false

# remotes::install_github("cct-datascience/azmetr")
library(azmetr)
library(tsibble)
library(tidyverse)
library(lubridate)
library(feasts)
library(fable)
library(slider)
```

# Problem Definition

We want to use forecasts to do quality assurance of AZMet weather data.
Use the existing timeseries available from the API (and possibly also historical data not on the API) to forecast the current day's (or hour's) data with prediction interval(s).
Data that falls outside of those prediction interval(s) will get flagged as extreme values and possibly interpolated.
Variables that need QA include:

-   precipitation
-   air temperature
-   soil temperature
-   solar radiation
-   wind speed
-   humidity

Other variables are (probably?) derived

# Gathering Information

## Retrieve Data

Download all the daily data available from the API.

```{r import-data}
#| cache: true
#| code-fold: false

#read in historic data
daily_hist <- read_csv("data/daily_hist.csv")

#get data from end of historic data until 2022-10-19
if(!file.exists("data/daily-2022-10-19.rds")) {
  daily <- az_daily(start_date = max(daily_hist$datetime) + 1,
                    end_date = "2022-10-19")
  
  #save these data
  write_rds(daily, "data/daily-2022-10-19.rds")
}

# Read in already existing data, extract last date, retrieve data since that date
daily_old <- read_rds("data/daily-2022-10-19.rds")
lastdate <- max(daily_old$datetime)

daily_new <- az_daily(start_date = lastdate + 1)

daily <- bind_rows(daily_old, daily_new)

#join to historic data

daily <- bind_rows(daily_hist, daily) 
```

Find and resolve duplicates

```{r}
duplicates(daily, key = c(meta_station_id, meta_station_name), index = datetime)

daily <- daily |>
  filter(!are_duplicated(
    daily,
    key = c(meta_station_id, meta_station_name),
    index = datetime
  ))
```

Convert to tsibble for exploration

```{r}
#| code-fold: false
daily_ts <- as_tsibble(daily, key = c(meta_station_id, meta_station_name), index = datetime)
```

Any rows marked as needing review?

```{r}
daily_ts |> filter(meta_needs_review != 0)
```

Check extreme values:

```{r}
daily_ts |> filter(temp_air_meanC > 200)
```

Looks like historic data uses 999 for NA probably.
I'll set those to `NA`

```{r}
daily_ts <- daily_ts |> 
  mutate(across(where(is.numeric), \(x) if_else(x == 999, NA_real_, x)))
```

Any gaps in the data?

```{r}
daily_ts |> scan_gaps()
```

yes, let's make them explicit NAs

```{r}
daily_ts <- fill_gaps(daily_ts, .full = TRUE)
```

Check that gaps were made explicit:

```{r}
#| code-fold: true
daily_ts |> 
  filter(meta_station_id %in% c("az23", "az14"))|>
  filter(year(datetime) > 2018) |> 
  ggplot(aes(x = datetime, y = sol_rad_total)) +
  geom_line(na.rm = TRUE, size = 0.1) +
  geom_point(na.rm = TRUE, size = 0.2) +
  facet_wrap(~meta_station_id)
```

# Preliminary (exploratory) analysis

I'll start by looking at a subset of sites just to make visualization easier.

```{r}
daily_ts_sub <-
  daily_ts |> 
  filter(meta_station_name %in% c("Aguila", "Harquahala", "Tucson", "Maricopa"))
```

Is there missing data?

```{r}
#| fig-height: 10
#| fig-width: 10

daily_ts_sub |> 
  as_tibble() |>
  group_by(meta_station_id, meta_station_name) |> 
  summarize(across(everything(), ~sum(is.na(.))))

```

Some variables are incomplete or have a short time gap.
This may make forecasting difficult.

## Mean Air Temp

```{r}
daily_ts_sub |> 
  autoplot(temp_air_meanC) +
  labs(title = "Mean Air Temp (ºC)")
```

Seasonality

```{r}
daily_ts_sub |> gg_season(temp_air_meanC) + labs(title = "Mean Air Temp (ºC)")
```

Autocorrelation

```{r}
daily_ts_sub |>
  ACF(temp_air_meanC, lag_max = 180) |> 
  autoplot()
```

## Solar Radiation

```{r}
daily_ts_sub |> autoplot(sol_rad_total) + labs(title = "Total Solar Radiation")
```

Seasonality

```{r}
daily_ts_sub |> gg_season(sol_rad_total) + labs(title = "Total Solar Radiation")
```

Definitely some weird zeroes.
Maybe a super cloudy day, but probably errors.

```{r}
#| code-fold: true
daily_ts |> 
  filter(sol_rad_total < 1 & !is.na(sol_rad_total)) |> 
  select(datetime, sol_rad_total, meta_station_id, meta_needs_review) |> 
  arrange(sol_rad_total)
```

Autocorrelation

```{r}
daily_ts_sub |>
  ACF(sol_rad_total, lag_max = 180) |> 
  autoplot()
```

## Precipitation

```{r}
daily_ts_sub |> autoplot(precip_total_mm) + labs(title = "Precip (mm)")
```

Seasonality

```{r}
daily_ts_sub |> gg_season(precip_total_mm) + labs(title = "Precip (mm)")
```

Autocorrelation

```{r}
daily_ts_sub |>
  ACF(precip_total_mm, lag_max = 180) |> 
  autoplot()
```

# Choosing and fitting models

## Sliding windows

I'll explore the sliding window quantile approach used in [@faybishenko2021].
I'll use `slider` because it's tidyverse-friendly and I want to learn it.
I'll use a rolling, centered, 6-month window to calculate upper and lower 95% and 99% quantiles.
Anything outside of the rolling 99% quantile is an "extreme" value, which @faybishenko2021 assumed was bad data, and anything between the 95% and 99% quantile is an "outlier" which is suspect, but not necessarily bad.

In the plots below, I only show a few stations for clarity.

### Mean Air Temp

```{r}
#| code-fold: true
temp_roll_test <-
  daily_ts |> 
  select(temp_air_meanC) |> 
  group_by_key() |> 
  mutate(
    rolling_median = slide_dbl(
      temp_air_meanC,
      median,
      .before = 90,
      .after  = 90,
      .complete = TRUE
    )
  ) |>
  mutate(upper95 = slide_dbl(
    temp_air_meanC,
    \(x) quantile(x, c(0.975), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |>
  mutate(lower95 = slide_dbl(
    temp_air_meanC,
    \(x) quantile(x, c(0.025), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(upper99 = slide_dbl(
    temp_air_meanC,
    \(x) quantile(x, c(0.999), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(lower99 = slide_dbl(
    temp_air_meanC,
    \(x) quantile(x, c(0.001), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(QA = case_when(
    temp_air_meanC > lower99 & temp_air_meanC < lower95 ~ "outlier",
    temp_air_meanC > upper95 & temp_air_meanC < upper99 ~ "outlier",
    temp_air_meanC < lower99 ~ "extreme",
    temp_air_meanC > upper99 ~ "extreme",
    is.na(temp_air_meanC) ~ NA_character_
  )) 
```

```{r}
#| code-fold: true
temp_roll_test |> 
 filter(meta_station_name %in% c("Bowie", "Harquahala", "Tucson", "Maricopa")) |>
  ggplot(aes(x = datetime)) +
  geom_point(aes(y = temp_air_meanC, color = QA), size = 0.5, alpha = 0.5) +
  geom_line(aes(y = rolling_median)) +
  geom_ribbon(aes(ymin = lower95, ymax = upper95), alpha = 0.3) +
  geom_ribbon(aes(ymin = lower99, ymax = upper99), alpha = 0.3) +
  facet_wrap(~meta_station_name)
```

### Solar radiation

```{r}
#| code-fold: true

sol_roll_test <-
  daily_ts |> 
  group_by_key() |>
  select(sol_rad_total) |>
  mutate(
    sol_rolling_median = slide_dbl(
      sol_rad_total,
      median,
      .before = 90,
      .after  = 90,
      .complete = TRUE
    )
  ) |>
  mutate(sol_upper95 = slide_dbl(
    sol_rad_total,
    \(x) quantile(x, c(0.975), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |>
  mutate(sol_lower95 = slide_dbl(
    sol_rad_total,
    \(x) quantile(x, c(0.025), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(sol_upper99 = slide_dbl(
    sol_rad_total,
    \(x) quantile(x, c(0.999), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(sol_lower99 = slide_dbl(
    sol_rad_total,
    \(x) quantile(x, c(0.001), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(QA = case_when(
    sol_rad_total > sol_lower99 & sol_rad_total < sol_lower95 ~ "outlier",
    sol_rad_total > sol_upper95 & sol_rad_total < sol_upper99 ~ "outlier",
    sol_rad_total < sol_lower99 ~ "extreme",
    sol_rad_total > sol_upper99 ~ "extreme",
    is.na(sol_rad_total) ~ NA_character_
  )) 

```

```{r}
#| code-fold: true
sol_roll_test |> 
 filter(meta_station_name %in% c("Bowie", "Harquahala", "Tucson", "Maricopa")) |>
  ggplot(aes(x = datetime)) +
  geom_point(aes(y = sol_rad_total, color = QA), size = 0.5) +
  geom_line(aes(y = sol_rolling_median)) +
  geom_ribbon(aes(ymin = sol_lower95, ymax = sol_upper95), alpha = 0.3) +
  geom_ribbon(aes(ymin = sol_lower99, ymax = sol_upper99), alpha = 0.3) +
  facet_wrap(~meta_station_name)
```

### Precipitation

```{r}
#| code-fold: true
precip_roll_test <-
  daily_ts |> 
  select(precip_total_mm) |> 
  group_by_key() |> 
  mutate(
    precip_rolling_median = slide_dbl(
      precip_total_mm,
      median,
      .before = 90,
      .after  = 90,
      .complete = TRUE
    )
  ) |>
  mutate(precip_upper95 = slide_dbl(
    precip_total_mm,
    \(x) quantile(x, c(0.975), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |>
  mutate(precip_lower95 = slide_dbl(
    precip_total_mm,
    \(x) quantile(x, c(0.025), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(precip_upper99 = slide_dbl(
    precip_total_mm,
    \(x) quantile(x, c(0.999), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(precip_lower99 = slide_dbl(
    precip_total_mm,
    \(x) quantile(x, c(0.001), na.rm = TRUE),
    .before = 90,
    .after  = 90,
    .complete = TRUE
  )) |> 
  mutate(QA = case_when(
    precip_total_mm > precip_lower99 & precip_total_mm < precip_lower95 ~ "outlier",
    precip_total_mm > precip_upper95 & precip_total_mm < precip_upper99 ~ "outlier",
    precip_total_mm < precip_lower99 ~ "extreme",
    precip_total_mm > precip_upper99 ~ "extreme",
    is.na(precip_total_mm) ~ NA_character_
  )) 
```

```{r}
#| code-fold: true
precip_roll_test |> 
 filter(meta_station_name %in% c("Bowie", "Harquahala", "Tucson", "Maricopa")) |>
  ggplot(aes(x = datetime)) +
  geom_point(aes(y = precip_total_mm, color = QA), size = 0.5) +
  # geom_line(aes(y = precip_rolling_median)) +
  geom_ribbon(aes(ymin = precip_lower95, ymax = precip_upper95), alpha = 0.3) +
  geom_ribbon(aes(ymin = precip_lower99, ymax = precip_upper99), alpha = 0.3) +
  facet_wrap(~meta_station_name)
```

Seems a little overzealous with precip.
Likely because an emprical distribution is not really appropriate for precip data.

## Forecasting

### Timeseries decomposition

Timeseries decomposition doesn't work when there is missing data.
There are `NA`s for Harquahala and Bowie stations, so I guess I'll have to omit those until I figure out how to do this with missing data.

```{r}
daily_ts_sub |> filter(is.na(sol_rad_total))
```

```{r}
dcmp <- 
  daily_ts_sub |> 
  # filter(meta_station_id == first(meta_station_id)) |> 
  model(stl = STL(sol_rad_total~season("1_year"), iterations = 3)) |> 
  filter(meta_station_id %in% c("az01", "az06"))
components(dcmp)
```

::: callout-note
`STL()` is only one possible algorithm (?) for timeseries decomposition.
`fable::ETS()` is another example.
:::

```{r}
components(dcmp) |>
  as_tsibble() |>
  autoplot(sol_rad_total, color = "grey") +
  facet_wrap(~meta_station_id + meta_station_name, ncol = 1) +
  geom_line(aes(y = trend, color = "trend")) 

components(dcmp) |> autoplot()
```

Other STL variations

```{r}
daily_ts_sub |> 
  filter(meta_station_id == first(meta_station_id)) |>
  model(
    STL(sol_rad_total ~ trend() + 
                        season(period = "1_year"), 
        robust = TRUE)) |> 
  components() |> 
  autoplot()
```

### Timeseries Features

```{r}
daily_ts_sub |> 
  features(sol_rad_total, list(
    mean = \(x) mean(x, na.rm = TRUE)
  )) |> arrange(mean)
```

```{r}
daily_ts_sub |> features(sol_rad_total, feat_acf)
daily_ts_sub |> features(sol_rad_total, feat_stl) 

#can't figure out how to set period correctly
daily_ts_sub |> 
  features(sol_rad_total, features = list(\(x) feat_stl(x, .period = "1 year")))

```

### Forecasting

Train a model.
IN this case, a simple timeseries linear model (TSLM)

```{r}
fit_tslm <- daily_ts_sub |> 
  model(TSLM(sol_rad_total~ trend())) 
fit_tslm
```
A slightly fancier model: seasonal naïve

```{r}
fit_snaive <- daily_ts_sub |> 
  model(SNAIVE(sol_rad_total ~ lag("1 year")))
fit_snaive
```


Produce forecasts

```{r}
fit_tslm |> forecast(h = "3 months")
fit_tslm |> 
  forecast(h = "3 months") |> 
  filter(meta_station_name == "Tucson") |> 
  autoplot(daily_ts_sub |> filter(year(datetime)>2021))
```

A very bad forecast because there is no trend here.

What about "seasonal naïve"

```{r}
fit_snaive |> 
  forecast(h = "1 month") |> 
  filter(meta_station_name == "Tucson") |> 
  autoplot(daily_ts_sub |> filter(year(datetime)>2021))
```

OOf, those prediction inteverals are WIDE.

Check residuals

```{r}
augment(fit_snaive) |> 
  ggplot(aes(.innov)) +
  geom_histogram()
```

residuals are leptokurtic and should be normal!

```{r}
augment(fit_snaive) |> 
  ACF(.innov) |> autoplot()
```

residuals are autocorrelated and they shouldn't be!

```{r}
fit_snaive |> filter(meta_station_id == "az01") |> gg_tsresiduals()
```
Could use bootstrapped residuals.

```{r}
fc <- fit_snaive |> forecast(h = "1 month", bootstrap = TRUE)
fc
```
```{r}
fc |> 
  filter(meta_station_name == "Tucson") |> 
  autoplot(daily_ts_sub |> filter(year(datetime)>2021))
```


Can do forecasting after decomposition

```{r}
fit_dcmp <-
  daily_ts_sub |> 
  model(stlf = decomposition_model(
    STL(sol_rad_total ~ season(period = "1 year")),
    NAIVE(season_adjust)
  ))
fit_dcmp |> 
  forecast(h = "1 month") |> 
  filter(meta_station_id ==  "az01") |> 
  autoplot(daily_ts_sub |> filter(year(datetime)>2021))
```

```{r}
fit_dcmp |> filter(meta_station_id == "az01") |> gg_tsresiduals()
```

Better??

Let's find out

```{r}
sol_rad_train <- 
  daily_ts_sub |> 
  filter(meta_station_id == "az01") |> 
  filter(year(datetime) < 2022) |> 
  select(sol_rad_total)

sol_rad_test <- 
  daily_ts_sub |> 
  filter(meta_station_id == "az01") |> 
  filter(year(datetime) >= 2022) |> 
  select(sol_rad_total)

fit_compare <- 
  sol_rad_train |> 
  model(
    mean = MEAN(sol_rad_total),
    naive = NAIVE(sol_rad_total),
    snaive = SNAIVE(sol_rad_total ~ lag("1 year")),
    drift = RW(sol_rad_total ~ drift()),
    stlf = decomposition_model(
      STL(sol_rad_total ~ season(period = "1 year")),
      NAIVE(season_adjust)
    )
  )

fc <- fit_compare |> forecast(sol_rad_test)
fc |> 
  autoplot(bind_rows(sol_rad_train, sol_rad_test) |> filter(year(datetime)>=2021), level = NULL)
accuracy(fc, sol_rad_test)

#winkler score evaluates prediction interval accuracy

accuracy(fc, sol_rad_test, list(winkler = winkler_score), level = 80)
```

best method here is seasonal naive


# QA workflow

0. Choose and validate models for each variable (see [#Forecasting])

1. Fit model to all data but most recent day

```{r}
sol_hist <- 
  daily_ts_sub |> 
  select(sol_rad_total) |> 
  filter(datetime < max(datetime))

sol_new <-
  daily_ts_sub |> 
  select(sol_rad_total) |> 
  filter(datetime == max(datetime))

sol_fit <- #TODO construct for a list of variables 
  sol_hist |> 
  model(sol = SNAIVE(sol_rad_total ~ lag("1 year")))

```

2. Forecast to new data

```{r}
sol_fc <- 
  sol_fit |> 
  forecast(sol_new, bootstrap = TRUE)
```

3. Check if data is inside prediction intervals

```{r}
flags <-
  left_join(as_tibble(sol_new), as_tibble(sol_fc |> hilo() |> select(-sol_rad_total))) |> 
  # make an outlier for testing
  mutate(sol_rad_total = if_else(meta_station_id == "az01", 10, sol_rad_total)) |> 
  mutate(
    outlier = if_else(sol_rad_total < `80%`$lower | sol_rad_total > `80%`$upper,
                      "sol_rad_total",
                      NA_character_),
    extreme = if_else(sol_rad_total < `95%`$lower | sol_rad_total > `95%`$upper,
                      "sol_rad_total",
                      NA_character_)
         ) |> 
  select(meta_station_id, meta_station_name, datetime, sol_rad_total, outlier, extreme)
flags
```

4.  Join back into data??

Here we'd need to deal with the possibility of multiple flags per observation

```{r}
left_join(daily_ts_sub, flags, by = c("meta_station_id", "meta_station_name", "datetime")) |> 
  arrange(desc(datetime)) |> 
  select(outlier, extreme, everything())
```

